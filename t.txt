Web Scraping Tool Assignment IMPORTANT: Please invite pawan18092019@gmail.com to your Git Hub repository after finishing the project. Please direct any questions to hr@unicraft.tech and send your final response. Overview Build a web scraping tool that, given a search query or a set of seed URLs, discovers companies and extracts detailed information. The tool should gather company name, website, contact information, technology stack, current projects, competitors, and other relevant details. You may choose to implement only the core (minimal) features or expand your solution with optional enhancements Instructions e Flexibility: You are encouraged to cherry-pick features based on your personal competency and skill set. Whether you implement just the minimal requirements or add optional enhancements, the goal is to demonstrate your strengths. e Feature Selection: o Minimal Requirements: Implement the core functionality that guarantees the tool has a clear purpose. o Optional Requirements: Choose any additional features that you feel comfortable with—from simple improvements to comprehensive, advanced capabilities. e Technology Choice: Use any programming language, libraries, or frameworks you prefer. This assignment is technology-agnostic. e Documentation: Include a README that details: o Which features and extraction levels do you implement? o Setup and execution instructions. o Any design decisions or assumptions you made. Minimal Requirements (Core Features) Implement at least the following core features: 1. Input Handling and Query Execution: o Accept a user-provided search query (e.g., “cloud computing startups in Europe”) or a set of seed URLs. o Validate inputs for proper URL formatting and reachability. 2. Basic Data Extraction (Level 1 — Basic): o Extract the following details for each company: = Company Name = Website URL = Basic Contact Information: Such as email addresses or phone numbers (if available). © Output: Store or display the extracted data in a structured format (e.g., CSV, JSON, or similar). 3. Error Handling: o Gracefully manage common issues such as network errors or missing data. o Log or display clear error messages when pages can not be processed. The minimal requirements ensure that even a simple solution demonstrates a clear purpose and functionality.Optional Requirements (Enhancement Features) For additional credit, consider implementing any of the following optional features based on your expertise: A. Data Extraction — Additional Levels 1. Medium Data Extraction (Level 2 - Enhanced Details): o Extended Contact Information: = Social media profiles (e.g., Linkedin, Twitter). m= Physical address or location details. o Company Overview: m= Brief description or tagline. m= Year founded or operational status. o Lead-Generation Specifics: m= Primary products or services offered. m= Industry or market sector. o Output: Organize the additional fields clearly in your chosen output format. 2. Advanced Data Extraction (Level 3 —- Comprehensive Insights): o Technical & Operational Details: m= Tech Stack: Detailed information on the technologies, frameworks, or tools the company uses. = Current Projects/Focus Areas: Information on active projects or innovative initiatives. o Competitive Landscape: = Competitors: List competitors or similar companies mentioned on the site. = Market Positioning: Insights into the company’s positioning (e.g., leader, challenger). o Data Enrichment: = Optionally integrate with external APIs {e.g., Linked In, Clearbit} to augment data with details such as company size, funding stage, or market performance.o Output: Produce a rich, multidimensional dataset that offers comprehensive insights for lead generation. B. Other Optional Enhancements 1. Dynamic Content Handling: o Use headless browsers (e.g., Puppeteer or Selenium) to scrape pages with Java Script-rendered content. o Implement wait mechanisms to ensure dynamic elements load before extraction. 2. Pagination and URL Discovery: o Automatically navigate through paginated listings. © Implement a simple crawler to discover additional relevant pages from seed URLs or sitemaps. 3. Customization and Flexibility: o Allow configuration of CSS selectors, XPath expressions, or regex patterns via a configuration file or command-line parameters. o Provide a simple CLI or web interface for user inputs and configuration. 4. Rate Limiting and Proxy Handling: o Implement rate limiting or IP rotation to avoid being blocked by target sites. o Use request delays to mimic human-like browsing behavior. 5. User Interface and Scheduling: o Develop a simple web dashboard or enhance your CLI to start, monitor, and schedule scraping jobs. o Display real-time progress or summary statistics upon completion. 6. Testing and Logging: o Write unit or integration tests for key functionalities. o Implement logging to record the scraping process, including successes and any encountered errors. Guidelines for Submission © Repository: Submit your project via a version-controlled repository (e.g., Git Hub, Git Lab) with a clear commit history. e Documentation: Include a README that explains: o Which features have you implemented? o Which data extraction levels (Basic, Medium, Advanced) do you choose to demonstrate? o Howto set up and run your tool. o Any design decisions or assumptions you made. e Output Sample: Provide sample output (e.g., a CSV or JSON file) generated by your tool. e Testing: Ensure that your tests (if implemented) can be run easily (e.g., via a single command). Evaluation Criteria e Clarity and Simplicity: The tool should be easy to run and understand, regardless of the number of features implemented. e Functionality: Demonstrate that your tool reliably extracts useful information at least at the Basic level, with optional enhancements as per your chosen complexity. e@ Code Quality: Maintain clean, well-documented code, and include a README that outlines your design decisions and usage instructions. e Creativity and Initiative: Extra features and innovative solutions are encouraged, whether they are simple or advanced. We look forward to seeing your creative and effective solutions! Good luck, and happy coding!

